{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc44fcdc",
   "metadata": {},
   "source": [
    "# Compression d'images en couleurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6bc56b",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons utiliser une sous partie du dataset ImageNet: https://www.kaggle.com/datasets/deeptrial/miniimagenet/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19b54e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fredl\\Documents\\Cours\\M1 Androide\\S2\\ML\\Projet\\src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredl\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cba817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fredl\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\fredl\\.cache\\kagglehub\\datasets\\deeptrial\\miniimagenet\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"deeptrial/miniimagenet\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3034c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from activation_func import *\n",
    "from module import *\n",
    "from loss import *\n",
    "from optimizers import *\n",
    "from mltools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca39658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3923 images from the dataset.\n"
     ]
    }
   ],
   "source": [
    "image_files = []\n",
    "for root, dirs, files in os.walk(os.path.join(path, \"ImageNet-Mini\", \"images\")):\n",
    "    for file in files:\n",
    "        if file.endswith(('.png', '.jpg', '.jpeg', '.JPEG')):\n",
    "            image_files.append(os.path.join(root, file))\n",
    "\n",
    "images = [Image.open(img_file) for img_file in image_files]\n",
    "\n",
    "print(f\"Loaded {len(images)} images from the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96150ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized 3923 images to (224, 224).\n"
     ]
    }
   ],
   "source": [
    "fixed_size = (224, 224)\n",
    "resized_images = [img.resize(fixed_size) for img in images]\n",
    "\n",
    "print(f\"Resized {len(resized_images)} images to {fixed_size}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b62c10",
   "metadata": {},
   "source": [
    "### Convertion des images en RGB, HSV et OKLCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344391d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2hsv, rgb2lab\n",
    "\n",
    "def preprocess_images(images):\n",
    "    rgb_images = [np.array(img) / 255.0 for img in images]  # Normalisation entre 0 et 1\n",
    "    # hsv_images = [rgb2hsv(img) for img in rgb_images if img.shape[-1] == 3]\n",
    "    # oklch_images = [rgb2lab(img) for img in rgb_images]  # TODO: LAB est utilisé comme placeholder pour OKLCH, à remplacer\n",
    "    # return rgb_images, hsv_images, oklch_images\n",
    "    return rgb_images\n",
    "\n",
    "# Normaliser les valeurs entre -1 et 1\n",
    "def normalize_images(images):\n",
    "    return [(img * 2 - 1) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0815ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les données\n",
    "# rgb_images, hsv_images, oklch_images = preprocess_images(resized_images)\n",
    "rgb_images = preprocess_images(resized_images)\n",
    "# hsv_images = normalize_images(hsv_images)\n",
    "# oklch_images = normalize_images(oklch_images)\n",
    "\n",
    "# Aplatir les images pour les passer dans l'autoencodeur\n",
    "rgb_flattened = [img.flatten() for img in rgb_images]\n",
    "# hsv_flattened = [img.flatten() for img in hsv_images]\n",
    "# oklch_flattened = [img.flatten() for img in oklch_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599aba7",
   "metadata": {},
   "source": [
    "### Expérience 1 : Trois autoencodeurs distincts pour R, G et B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80be988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_channel = fixed_size[0] * fixed_size[1]\n",
    "\n",
    "autoencoder_r = AutoEncoder([input_dim_channel, 8192, 4096, 1024], [1024, 4096, 8192, input_dim_channel])\n",
    "autoencoder_g = AutoEncoder([input_dim_channel, 8192, 4096, 1024], [1024, 4096, 8192, input_dim_channel])\n",
    "autoencoder_b = AutoEncoder([input_dim_channel, 8192, 4096, 1024], [1024, 4096, 8192, input_dim_channel])\n",
    "\n",
    "r_channel = [img[:, :, 0].flatten() for img in rgb_images if img.ndim == 3]\n",
    "g_channel = [img[:, :, 1].flatten() for img in rgb_images if img.ndim == 3]\n",
    "b_channel = [img[:, :, 2].flatten() for img in rgb_images if img.ndim == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eac6272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R Channel Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining R Channel Autoencoder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m optimizer_r \u001b[38;5;241m=\u001b[39m Optim(autoencoder_r, loss_fn, learning_rate)\n\u001b[1;32m----> 8\u001b[0m losses_r \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_r\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining G Channel Autoencoder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m optimizer_g \u001b[38;5;241m=\u001b[39m Optim(autoencoder_g, loss_fn, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\fredl\\Documents\\Cours\\M1 Androide\\S2\\ML\\Projet\\src\\optimizers.py:25\u001b[0m, in \u001b[0;36mOptim.SGD\u001b[1;34m(self, data_x, data_y, batch_size, num_iterations)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data_x), batch_size):\n\u001b[0;32m     24\u001b[0m     batch_indices \u001b[38;5;241m=\u001b[39m indices[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m---> 25\u001b[0m     batch_x \u001b[38;5;241m=\u001b[39m \u001b[43mdata_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     26\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m data_y[batch_indices]\n\u001b[0;32m     27\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(batch_x, batch_y))\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "loss_fn = MSELoss()\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "print(\"Training R Channel Autoencoder...\")\n",
    "optimizer_r = Optim(autoencoder_r, loss_fn, learning_rate)\n",
    "losses_r = optimizer_r.SGD(r_channel, r_channel, batch_size=batch_size, num_iterations=epochs)\n",
    "\n",
    "print(\"Training G Channel Autoencoder...\")\n",
    "optimizer_g = Optim(autoencoder_g, loss_fn, learning_rate)\n",
    "losses_g = optimizer_g.SGD(g_channel, g_channel, batch_size=batch_size, num_iterations=epochs)\n",
    "\n",
    "print(\"Training B Channel Autoencoder...\")\n",
    "optimizer_b = Optim(autoencoder_b, loss_fn, learning_rate)\n",
    "losses_b = optimizer_b.SGD(b_channel, b_channel, batch_size=batch_size, num_iterations=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(losses_r, label='R Channel Loss')\n",
    "plt.plot(losses_g, label='G Channel Loss')\n",
    "plt.plot(losses_b, label='B Channel Loss')\n",
    "plt.title('Training Losses for Each Channel Autoencoder')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def plot_reconstructed_images(original, reconstructed, channel_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(5):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(original[i].reshape(fixed_size[0], fixed_size[1]), cmap='gray')\n",
    "        plt.title(f'Original {channel_name}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(2, 5, i + 6)\n",
    "        plt.imshow(reconstructed[i].reshape(fixed_size[0], fixed_size[1]), cmap='gray')\n",
    "        plt.title(f'Reconstructed {channel_name}')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "reconstructed_r = [autoencoder_r.forward(img) for img in r_channel]\n",
    "reconstructed_g = [autoencoder_g.forward(img) for img in g_channel]\n",
    "reconstructed_b = [autoencoder_b.forward(img) for img in b_channel]\n",
    "\n",
    "plot_reconstructed_images(r_channel, reconstructed_r, 'R Channel')\n",
    "plot_reconstructed_images(g_channel, reconstructed_g, 'G Channel')\n",
    "plot_reconstructed_images(b_channel, reconstructed_b, 'B Channel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
